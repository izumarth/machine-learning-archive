# -*- coding: utf-8 -*-
"""Undersampling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/189ooZarob5pnrruBc3Cs9kKMpAKNCQb3
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from collections import Counter

from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, average_precision_score
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.preprocessing import MinMaxScaler

from imblearn.under_sampling import RandomUnderSampler
from imblearn.datasets import fetch_datasets

from imblearn.pipeline import make_pipeline

from imblearn.under_sampling import CondensedNearestNeighbour
from imblearn.under_sampling import TomekLinks

def make_data(sep):

    X, y = make_classification(n_samples=1000,
                           n_features=2,
                           n_redundant=0,
                           n_clusters_per_class=1,
                           weights=[0.99],
                           class_sep=sep,
                           random_state=1)

    X = pd.DataFrame(X, columns =['varA', 'varB'])
    y = pd.Series(y)

    return X, y

# Load kdd
data = fetch_datasets()['protein_homo']

data = pd.concat([
    pd.DataFrame(data.data),
    pd.Series(data.target)
], axis=1)

col_names = [str(i) for i in range(74)] +['target']
data.columns = col_names

data.head()

for sep in [0.1, 1., 2.]:
  X,y = make_data(sep)
  print(y.value_counts())

  sns.scatterplot(
      data=X, x="varA", y="varB", hue=y,
  )

  plt.title(f'Separation: {sep}')
  plt.show()

"""## RandomUnserSampling"""

X,y = make_data(sep=2)

rus = RandomUnderSampler(
    sampling_strategy='auto',
    random_state=0,
    replacement=True,
)

X_resampled, y_resampled = rus.fit_resample(X, y)

X.shape, y.shape

X_resampled.shape, y_resampled.shape

sns.scatterplot(
    data=X, x="varA", y="varB", hue=y,
)

plt.title(f'Original: {sep}')
plt.show()

sns.scatterplot(
    data=X_resampled, x="varA", y="varB", hue=y,
)

plt.title(f'UnderSampling: {sep}')
plt.show()

"""# Kdd dataset"""

data.target.value_counts() / len(data)

X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=42
)

X_train.shape, X_test.shape

rus = RandomUnderSampler(
    sampling_strategy='auto',
    random_state=0,
    replacement=True,
)

X_resampled, y_resampled = rus.fit_resample(X_train, y_train)

X_resampled.shape, y_resampled.shape

col_names = [str(i) for i in range(74)] +['target']

data_resampled = pd.concat([X_resampled, y_resampled], axis=1)
data_resampled.columns = col_names

sns.scatterplot(data=data_resampled, x="0", y="1", hue="target")

# function to train random forests and evaluate the performance
from sklearn.metrics import confusion_matrix

def run_randomForests(X_train, X_test, y_train, y_test):

    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)
    rf.fit(X_train, y_train)

    print('Train set')
    pred = rf.predict_proba(X_train)
    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))

    print('Test set')
    pred = rf.predict_proba(X_test)
    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))

    cm = confusion_matrix(y_test, rf.predict(X_test), labels=[-1, 1])

    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=['-1', '1'], yticklabels=['-1', '1'])

run_randomForests(X_train, X_test, y_train,y_test)

run_randomForests(X_resampled, X_test, y_resampled, y_test)

"""# Condensed Nearest Neighbours

-  境界線近くのデータをSamplingする
- kNNが絡むのでパフォーマンス向上もむずくかしく結構きびしい印象。。。
- 大量のkNNも実行されるので時間がめちゃくちゃかかる
"""

X, y = make_data(sep=2)


cnn = CondensedNearestNeighbour(
    sampling_strategy='auto',
    random_state=0,
    n_neighbors=1,
    n_jobs=4)

X_resampled, y_resampled = cnn.fit_resample(X, y)

X.shape, y.shape

X_resampled.shape, X_resampled.shape

sns.scatterplot(
    data=X_resampled, x="varA", y="varB", hue=y_resampled,
)

plt.title(f'UnderSampling: {sep}')
plt.show()

#kdd
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape

# Warning!! this process is very heavy!!!!!
cnn = CondensedNearestNeighbour(
    sampling_strategy='auto',  # sundersamples only the majority class
    random_state=0,  # for reproducibility
    n_neighbors=1,
    n_jobs=4)

X_resampled, y_resampled = cnn.fit_resample(X_train, y_train)

"""# Setting up a classifier with under-sampling and CV"""

datasets_ls = [
    'car_eval_34',
    'ecoli',
    'thyroid_sick',
    'arrhythmia',
    'ozone_level'
]

for dataset in datasets_ls:
    data = fetch_datasets()[dataset]
    print(dataset)
    print(Counter(data.target))
    print()

def run_model(X_train, y_train, undersampler=None):

    rf = RandomForestClassifier(
            n_estimators=100, random_state=39, max_depth=3, n_jobs=4
        )

    scaler = MinMaxScaler()

    if not undersampler:
        model = rf

    else:
        model = make_pipeline(
            scaler,
            undersampler,
            rf,
        )

    cv_results = cross_validate(
        model,
        X_train,
        y_train,
        scoring="average_precision",
        cv=3,
    )

    print(
        'Random Forests average precision: {0} +/- {1}'.format(
        cv_results['test_score'].mean(), cv_results['test_score'].std()
        )
    )

    return cv_results['test_score'].mean(), cv_results['test_score'].std()

rus = RandomUnderSampler(
        sampling_strategy='auto',
        random_state=0,
        replacement=False)

pr_mean_dict = {}
pr_std_dict = {}

for dataset in datasets_ls:

    pr_mean_dict[dataset] = {}
    pr_std_dict[dataset] = {}

    print(dataset)

    data = fetch_datasets()[dataset]

    X_train, X_test, y_train, y_test = train_test_split(
        data.data,
        data.target,
        test_size=0.3,
        random_state=0,
    )

    aps_mean, aps_std = run_model(X_train, y_train)
    pr_mean_dict[dataset]['full_data'] = aps_mean
    pr_std_dict[dataset]['full_data'] = aps_std

    aps_mean, aps_std = run_model(X_train, y_train, rus)

    pr_mean_dict[dataset]['rus'] = aps_mean
    pr_std_dict[dataset]['rus'] = aps_std

for dataset in datasets_ls:

    pr_mean_s = pd.Series(pr_mean_dict[dataset])
    pr_std_s = pd.Series(pr_std_dict[dataset])

    pr_mean_s.plot.bar(yerr=[pr_std_s, pr_std_s]
        )
    plt.title(dataset)
    plt.ylabel('Average Precision')
    plt.axhline(pr_mean_dict[dataset]['full_data'], color='r')
    plt.show()

